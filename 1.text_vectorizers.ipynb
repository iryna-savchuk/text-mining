{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43374b35",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cb043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/Users/isavchuk/projects/text_mining/venv/bin/python3 -m pip install -q sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568b4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ca8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\"Be yourself; everyone else is already taken.\",\n",
    "            \"A room without books is like a body without a soul.\",\n",
    "            \"Be the change that you wish to see in the world.\",\n",
    "            \"If you tell the truth, you don't need to remember anything.\",\n",
    "            \"Always forgive your enemies; nothing annoys them so much.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dde2bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'be': 4,\n",
       " 'yourself': 37,\n",
       " 'everyone': 11,\n",
       " 'else': 9,\n",
       " 'is': 15,\n",
       " 'already': 0,\n",
       " 'taken': 25,\n",
       " 'room': 21,\n",
       " 'without': 33,\n",
       " 'books': 6,\n",
       " 'like': 16,\n",
       " 'body': 5,\n",
       " 'soul': 24,\n",
       " 'the': 28,\n",
       " 'change': 7,\n",
       " 'that': 27,\n",
       " 'you': 35,\n",
       " 'wish': 32,\n",
       " 'to': 30,\n",
       " 'see': 22,\n",
       " 'in': 14,\n",
       " 'world': 34,\n",
       " 'if': 13,\n",
       " 'tell': 26,\n",
       " 'truth': 31,\n",
       " 'don': 8,\n",
       " 'need': 18,\n",
       " 'remember': 20,\n",
       " 'anything': 3,\n",
       " 'always': 1,\n",
       " 'forgive': 12,\n",
       " 'your': 36,\n",
       " 'enemies': 10,\n",
       " 'nothing': 19,\n",
       " 'annoys': 2,\n",
       " 'them': 29,\n",
       " 'so': 23,\n",
       " 'much': 17}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.fit(text_list)\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c2c75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_.get(\"soul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a24017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 38)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector = count_vectorizer.transform(text_list)\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "219c7a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 2, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22340338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.transform([\"a new text containing already be yourself\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f396c5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['already', 'be', 'else', 'everyone', 'is', 'taken', 'yourself'],\n",
       "       dtype='<U8'),\n",
       " array(['body', 'books', 'is', 'like', 'room', 'soul', 'without'],\n",
       "       dtype='<U8'),\n",
       " array(['be', 'change', 'in', 'see', 'that', 'the', 'to', 'wish', 'world',\n",
       "        'you'], dtype='<U8'),\n",
       " array(['anything', 'don', 'if', 'need', 'remember', 'tell', 'the', 'to',\n",
       "        'truth', 'you'], dtype='<U8'),\n",
       " array(['always', 'annoys', 'enemies', 'forgive', 'much', 'nothing', 'so',\n",
       "        'them', 'your'], dtype='<U8')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.inverse_transform(transformed_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f518985",
   "metadata": {},
   "source": [
    "## N-gramm Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23d3c830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 41)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "transformed_vector = n_gram_vectorizer.fit_transform(text_list)\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bf0c66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'be yourself': 4,\n",
       " 'yourself everyone': 40,\n",
       " 'everyone else': 11,\n",
       " 'else is': 9,\n",
       " 'is already': 15,\n",
       " 'already taken': 0,\n",
       " 'room without': 21,\n",
       " 'without books': 34,\n",
       " 'books is': 6,\n",
       " 'is like': 16,\n",
       " 'like body': 17,\n",
       " 'body without': 5,\n",
       " 'without soul': 35,\n",
       " 'be the': 3,\n",
       " 'the change': 26,\n",
       " 'change that': 7,\n",
       " 'that you': 25,\n",
       " 'you wish': 38,\n",
       " 'wish to': 33,\n",
       " 'to see': 31,\n",
       " 'see in': 22,\n",
       " 'in the': 14,\n",
       " 'the world': 28,\n",
       " 'if you': 13,\n",
       " 'you tell': 37,\n",
       " 'tell the': 24,\n",
       " 'the truth': 27,\n",
       " 'truth you': 32,\n",
       " 'you don': 36,\n",
       " 'don need': 8,\n",
       " 'need to': 18,\n",
       " 'to remember': 30,\n",
       " 'remember anything': 20,\n",
       " 'always forgive': 1,\n",
       " 'forgive your': 12,\n",
       " 'your enemies': 39,\n",
       " 'enemies nothing': 10,\n",
       " 'nothing annoys': 19,\n",
       " 'annoys them': 2,\n",
       " 'them so': 29,\n",
       " 'so much': 23}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8367b0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3efe46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['be yourself', 'yourself everyone', 'everyone else', 'else is',\n",
       "        'is already', 'already taken'], dtype='<U17'),\n",
       " array(['room without', 'without books', 'books is', 'is like',\n",
       "        'like body', 'body without', 'without soul'], dtype='<U17'),\n",
       " array(['be the', 'the change', 'change that', 'that you', 'you wish',\n",
       "        'wish to', 'to see', 'see in', 'in the', 'the world'], dtype='<U17'),\n",
       " array(['if you', 'you tell', 'tell the', 'the truth', 'truth you',\n",
       "        'you don', 'don need', 'need to', 'to remember',\n",
       "        'remember anything'], dtype='<U17'),\n",
       " array(['always forgive', 'forgive your', 'your enemies',\n",
       "        'enemies nothing', 'nothing annoys', 'annoys them', 'them so',\n",
       "        'so much'], dtype='<U17')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_vectorizer.inverse_transform(transformed_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32efb231",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d399adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3afe9c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 38)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector = tfidf_vectorizer.fit_transform(text_list)\n",
    "tfidf_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d9b3b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'be': 4,\n",
       " 'yourself': 37,\n",
       " 'everyone': 11,\n",
       " 'else': 9,\n",
       " 'is': 15,\n",
       " 'already': 0,\n",
       " 'taken': 25,\n",
       " 'room': 21,\n",
       " 'without': 33,\n",
       " 'books': 6,\n",
       " 'like': 16,\n",
       " 'body': 5,\n",
       " 'soul': 24,\n",
       " 'the': 28,\n",
       " 'change': 7,\n",
       " 'that': 27,\n",
       " 'you': 35,\n",
       " 'wish': 32,\n",
       " 'to': 30,\n",
       " 'see': 22,\n",
       " 'in': 14,\n",
       " 'world': 34,\n",
       " 'if': 13,\n",
       " 'tell': 26,\n",
       " 'truth': 31,\n",
       " 'don': 8,\n",
       " 'need': 18,\n",
       " 'remember': 20,\n",
       " 'anything': 3,\n",
       " 'always': 1,\n",
       " 'forgive': 12,\n",
       " 'your': 36,\n",
       " 'enemies': 10,\n",
       " 'nothing': 19,\n",
       " 'annoys': 2,\n",
       " 'them': 29,\n",
       " 'so': 23,\n",
       " 'much': 17}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38a42f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39835162, 0.        , 0.        , 0.        , 0.32138758,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.39835162,\n",
       "        0.        , 0.39835162, 0.        , 0.        , 0.        ,\n",
       "        0.32138758, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.39835162, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.39835162],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.32189611, 0.32189611, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25970376, 0.32189611, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.32189611, 0.        , 0.        , 0.32189611,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.64379222, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.24831578,\n",
       "        0.        , 0.        , 0.30778101, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30778101,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30778101, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30778101, 0.49663156, 0.        ,\n",
       "        0.24831578, 0.        , 0.30778101, 0.        , 0.30778101,\n",
       "        0.24831578, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.30281493, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30281493, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30281493, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30281493, 0.        ,\n",
       "        0.30281493, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.30281493, 0.        , 0.24430918, 0.        ,\n",
       "        0.24430918, 0.30281493, 0.        , 0.        , 0.        ,\n",
       "        0.48861835, 0.        , 0.        ],\n",
       "       [0.        , 0.33333333, 0.33333333, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.33333333, 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.33333333, 0.        , 0.33333333,\n",
       "        0.        , 0.        , 0.        , 0.33333333, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.33333333,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.33333333, 0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bd922c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09861229, 2.09861229, 2.09861229, 2.09861229, 1.69314718,\n",
       "       2.09861229, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       1.69314718, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       2.09861229, 2.09861229, 2.09861229, 1.69314718, 2.09861229,\n",
       "       1.69314718, 2.09861229, 2.09861229, 2.09861229, 2.09861229,\n",
       "       1.69314718, 2.09861229, 2.09861229])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "689788ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isavchuk/projects/text_mining/venv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'already': 2.09861228866811,\n",
       " 'always': 2.09861228866811,\n",
       " 'annoys': 2.09861228866811,\n",
       " 'anything': 2.09861228866811,\n",
       " 'be': 1.6931471805599454,\n",
       " 'body': 2.09861228866811,\n",
       " 'books': 2.09861228866811,\n",
       " 'change': 2.09861228866811,\n",
       " 'don': 2.09861228866811,\n",
       " 'else': 2.09861228866811,\n",
       " 'enemies': 2.09861228866811,\n",
       " 'everyone': 2.09861228866811,\n",
       " 'forgive': 2.09861228866811,\n",
       " 'if': 2.09861228866811,\n",
       " 'in': 2.09861228866811,\n",
       " 'is': 1.6931471805599454,\n",
       " 'like': 2.09861228866811,\n",
       " 'much': 2.09861228866811,\n",
       " 'need': 2.09861228866811,\n",
       " 'nothing': 2.09861228866811,\n",
       " 'remember': 2.09861228866811,\n",
       " 'room': 2.09861228866811,\n",
       " 'see': 2.09861228866811,\n",
       " 'so': 2.09861228866811,\n",
       " 'soul': 2.09861228866811,\n",
       " 'taken': 2.09861228866811,\n",
       " 'tell': 2.09861228866811,\n",
       " 'that': 2.09861228866811,\n",
       " 'the': 1.6931471805599454,\n",
       " 'them': 2.09861228866811,\n",
       " 'to': 1.6931471805599454,\n",
       " 'truth': 2.09861228866811,\n",
       " 'wish': 2.09861228866811,\n",
       " 'without': 2.09861228866811,\n",
       " 'world': 2.09861228866811,\n",
       " 'you': 1.6931471805599454,\n",
       " 'your': 2.09861228866811,\n",
       " 'yourself': 2.09861228866811}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(tfidf_vectorizer.get_feature_names(), tfidf_vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2464aeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['taken', 'already', 'is', 'else', 'everyone', 'yourself', 'be'],\n",
       "       dtype='<U8'),\n",
       " array(['soul', 'body', 'like', 'books', 'without', 'room', 'is'],\n",
       "       dtype='<U8'),\n",
       " array(['world', 'in', 'see', 'to', 'wish', 'you', 'that', 'change', 'the',\n",
       "        'be'], dtype='<U8'),\n",
       " array(['anything', 'remember', 'need', 'don', 'truth', 'tell', 'if', 'to',\n",
       "        'you', 'the'], dtype='<U8'),\n",
       " array(['much', 'so', 'them', 'annoys', 'nothing', 'enemies', 'your',\n",
       "        'forgive', 'always'], dtype='<U8')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.inverse_transform(tfidf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea76d0",
   "metadata": {},
   "source": [
    "## Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e79358cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  1.,  1.,  0.,  0., -2.,  1.,  0.,  0.,  0.,  0.,\n",
       "         1., -1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  2., -1.,  0.,  2.,  1.,  0.,  0.,\n",
       "         1., -1.,  0.],\n",
       "       [ 0.,  1.,  0.,  1.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,\n",
       "         0., -3., -1.],\n",
       "       [ 1.,  0.,  0., -1.,  0.,  0.,  2.,  0.,  0.,  1.,  0.,  1.,  2.,\n",
       "         1.,  0.,  0.],\n",
       "       [-1.,  0.,  0.,  0., -1.,  0.,  0., -2., -1.,  1.,  0.,  0.,  0.,\n",
       "         0., -1.,  0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashing_vectorizer = HashingVectorizer(n_features=16, norm=None)\n",
    "feature_vector = hashing_vectorizer.transform(text_list)\n",
    "\n",
    "print('shape:', feature_vector.shape) # each of 5 vectors has been hashed to 16 features\n",
    "feature_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aec88f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.14285714,  0.14285714,\n",
       "         0.        ,  0.        , -0.28571429,  0.14285714,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.14285714, -0.14285714,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.25      , -0.125     ,  0.        ,  0.25      ,\n",
       "         0.125     ,  0.        ,  0.        ,  0.125     , -0.125     ,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.11111111,  0.        ,  0.11111111,  0.        ,\n",
       "        -0.11111111,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.11111111,  0.11111111,  0.        , -0.33333333,\n",
       "        -0.11111111],\n",
       "       [ 0.11111111,  0.        ,  0.        , -0.11111111,  0.        ,\n",
       "         0.        ,  0.22222222,  0.        ,  0.        ,  0.11111111,\n",
       "         0.        ,  0.11111111,  0.22222222,  0.11111111,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.14285714,  0.        ,  0.        ,  0.        , -0.14285714,\n",
       "         0.        ,  0.        , -0.28571429, -0.14285714,  0.14285714,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , -0.14285714,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_vectorizer = HashingVectorizer(n_features=16, norm='l1') ## L1 norm\n",
    "feature_vector = hashing_vectorizer.transform(text_list)\n",
    "\n",
    "print('shape:', feature_vector.shape)\n",
    "feature_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5006f53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.33333333,  0.33333333,\n",
       "         0.        ,  0.        , -0.66666667,  0.33333333,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.33333333, -0.33333333,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.57735027, -0.28867513,  0.        ,  0.57735027,\n",
       "         0.28867513,  0.        ,  0.        ,  0.28867513, -0.28867513,\n",
       "         0.        ],\n",
       "       [ 0.        ,  0.25819889,  0.        ,  0.25819889,  0.        ,\n",
       "        -0.25819889,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.25819889,  0.25819889,  0.        , -0.77459667,\n",
       "        -0.25819889],\n",
       "       [ 0.2773501 ,  0.        ,  0.        , -0.2773501 ,  0.        ,\n",
       "         0.        ,  0.5547002 ,  0.        ,  0.        ,  0.2773501 ,\n",
       "         0.        ,  0.2773501 ,  0.5547002 ,  0.2773501 ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.33333333,  0.        ,  0.        ,  0.        , -0.33333333,\n",
       "         0.        ,  0.        , -0.66666667, -0.33333333,  0.33333333,\n",
       "         0.        ,  0.        ,  0.        ,  0.        , -0.33333333,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_vectorizer = HashingVectorizer(n_features=16, norm='l2') ## L2 norm\n",
    "feature_vector = hashing_vectorizer.transform(text_list)\n",
    "\n",
    "print('shape:', feature_vector.shape)\n",
    "feature_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a828f7fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HashingVectorizer' object has no attribute 'inverse_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Hashing is a one-way operation! \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mhashing_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m(feature_vector)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HashingVectorizer' object has no attribute 'inverse_transform'"
     ]
    }
   ],
   "source": [
    "# Hashing is a one-way operation! \n",
    "hashing_vectorizer.inverse_transform(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0af56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
