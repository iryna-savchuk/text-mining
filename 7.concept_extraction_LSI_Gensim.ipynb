{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "badb1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI (Latent Semantic Indexing) is an indexing and information retrieval method that uses \n",
    "# Singular Value Decomposition to identify relationships between terms and concepts in a collection of documents.\n",
    "\n",
    "# Extracts the conceptual content of a document by establising associations\n",
    "# between those terms that occur in similar cotexts - these terms are more likely to have similar meanings\n",
    "\n",
    "# Here, we will use Gensim, but the LSI model is also present in Sklearn:\n",
    "# sklearn.decomposition.TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db317aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/isavchuk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/isavchuk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/isavchuk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/isavchuk/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim is a free Python library to analyze plain text documents for semantic structure\n",
    "import gensim \n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet') # WordNET is a lexical database of words in more than 200 languages\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1003e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries and packages \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c34ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240942, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset - large multi-domain ontology derived from Wikipedia, can be found at:\n",
    "# https://www.kaggle.com/datasets/danofer/dbpedia-classes\n",
    "# http://dbpedia-generic.tib.eu/release/text/\n",
    "dbpedia_df = pd.read_csv('./datasets/dbpedia/DBPEDIA_train.csv')\n",
    "\n",
    "dbpedia_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99df241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9431</th>\n",
       "      <td>8776 Campestris, provisional designation 2287 ...</td>\n",
       "      <td>Place</td>\n",
       "      <td>CelestialBody</td>\n",
       "      <td>Planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7159</th>\n",
       "      <td>Les Capewell is a retired English professional...</td>\n",
       "      <td>Agent</td>\n",
       "      <td>Athlete</td>\n",
       "      <td>DartsPlayer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>Cteniza moggridgei is a spider species found i...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Animal</td>\n",
       "      <td>Arachnid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9676</th>\n",
       "      <td>The Zeitschrift für Ostmitteleuropa-Forschung ...</td>\n",
       "      <td>Work</td>\n",
       "      <td>PeriodicalLiterature</td>\n",
       "      <td>AcademicJournal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6793</th>\n",
       "      <td>The 1991 PGA Championship was the 73rd PGA Cha...</td>\n",
       "      <td>Event</td>\n",
       "      <td>Tournament</td>\n",
       "      <td>GolfTournament</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text       l1  \\\n",
       "9431  8776 Campestris, provisional designation 2287 ...    Place   \n",
       "7159  Les Capewell is a retired English professional...    Agent   \n",
       "2513  Cteniza moggridgei is a spider species found i...  Species   \n",
       "9676  The Zeitschrift für Ostmitteleuropa-Forschung ...     Work   \n",
       "6793  The 1991 PGA Championship was the 73rd PGA Cha...    Event   \n",
       "\n",
       "                        l2               l3  \n",
       "9431         CelestialBody           Planet  \n",
       "7159               Athlete      DartsPlayer  \n",
       "2513                Animal         Arachnid  \n",
       "9676  PeriodicalLiterature  AcademicJournal  \n",
       "6793            Tournament   GolfTournament  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_SAMPLES = 10000\n",
    "\n",
    "dbpedia_df = dbpedia_df.sample(NUM_SAMPLES, random_state=1000, replace=False).reset_index(drop=True)\n",
    "dbpedia_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4b175e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Piz de la Lumbreida is a mountain of the Lepon...\n",
       "1    The Men's 81 kg Judo competition at the 2008 S...\n",
       "2    21561 Masterman (1998 QR93) is a main-belt ast...\n",
       "3    The Knud Rasmussen Range (Danish: Knud Rasmuss...\n",
       "4    Richard \\\"Dick\\\" Ramsdale (birth registered Ap...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dbpedia_df['text']\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32264308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piz de la Lumbreida is a mountain of the Lepontine Alps, overlooking San Bernardino in the canton of Graubünden.\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing to remove numbers\n",
    "documents_list=[]\n",
    "\n",
    "for line in text:\n",
    "    sentence = line.strip()\n",
    "    new_sentence= re.sub(r\"\\d\",\"\",sentence)\n",
    "    \n",
    "    documents_list.append(new_sentence)\n",
    "    \n",
    "print(documents_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe86a92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_list) # we now have list of 10000 text documents, with all numbers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "254e2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update('.', ',', '\"', \"'\", '!', '?', ';', ':', '(', ')','[', ']', '{', '}', '#', '...', '--', \"'s\", 'also', \n",
    "                  '&', '-', '—', '=', 'known', 'mi', 'km', '$', \"''\", '\\\\', '*', '–', \"'s\", '\\\\n', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd7765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['piz', 'de', 'la', 'lumbreida', 'mountain', 'lepontine', 'alp', 'overlooking', 'san', 'bernardino', 'canton', 'graubünden']\n"
     ]
    }
   ],
   "source": [
    "processed_list = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for doc in documents_list:\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    stopped_tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(i, pos='n') for i in stopped_tokens]\n",
    "    processed_list.append(lemmatized_tokens)\n",
    "    \n",
    "print(processed_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b95cffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02888632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<66723 unique tokens: ['alp', 'bernardino', 'canton', 'de', 'graubünden']...>\n"
     ]
    }
   ],
   "source": [
    "term_dictionary = Dictionary(processed_list)\n",
    "print(term_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7159b472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_dictionary.token2id[\"maximum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eeec93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1747"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_dictionary.token2id[\"ridge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0b2dc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document-term matrix is a mathematical matrix that describes the frequency of terms\n",
    "# that occur in a set of documents\n",
    "document_term_matrix = [term_dictionary.doc2bow(document) \n",
    "                        for document in processed_list] #generate Bag-of-words representation\n",
    "len(document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e2ed380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john', 'warren', 'bettis', 'october', 'june', 'ohio', 'jurist', 'served', 'judge', 'ohio', 'court', 'claim']\n",
      "[(43, 1), (149, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 2), (170, 1), (171, 1)] \n",
      "\n",
      "List of words and document terms:  12 11\n",
      "Set of words and document terms:  11 11\n"
     ]
    }
   ],
   "source": [
    "# Let's have a look at one particular document\n",
    "print(processed_list[5])\n",
    "print(document_term_matrix[5], '\\n')\n",
    "print('List of words and document terms: ', len(processed_list[5]), len(document_term_matrix[5]))\n",
    "print('Set of words and document terms: ', len(set(processed_list[5])), len(document_term_matrix[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47cae1d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [(\"''\", 0.7868019465282237),\n",
       "   (\"'s\", 0.2949460633717554),\n",
       "   ('album', 0.15503679854182595),\n",
       "   ('first', 0.12029661732505383),\n",
       "   ('also', 0.11615194695443597),\n",
       "   ('single', 0.09729577077536046),\n",
       "   ('song', 0.0935425692511347),\n",
       "   ('one', 0.08980391920778537),\n",
       "   ('released', 0.08950381575486088),\n",
       "   ('year', 0.08113436708630277)]),\n",
       " (1,\n",
       "  [('\\\\n', 0.9329067589089908),\n",
       "   ('callistomimus', 0.16162991628371332),\n",
       "   ('nosodendron', 0.15446341113908774),\n",
       "   (\"''\", -0.12109436618298937),\n",
       "   ('hyalella', 0.08957836824909708),\n",
       "   ('arctodiaptomus', 0.08421716634244263),\n",
       "   ('ceraeochrysa', 0.08046257864131594),\n",
       "   ('geosesarma', 0.07480803196162861),\n",
       "   ('th', 0.04982636289752765),\n",
       "   ('division', 0.03929004531129575)]),\n",
       " (2,\n",
       "  [(\"''\", 0.47130589671823137),\n",
       "   (\"'s\", -0.30038389507087965),\n",
       "   ('team', -0.2252121031603723),\n",
       "   ('season', -0.22512358971487034),\n",
       "   ('league', -0.18238261649991783),\n",
       "   ('\\\\n', 0.1554818831827751),\n",
       "   ('first', -0.15207340076753897),\n",
       "   ('football', -0.14244063291369416),\n",
       "   ('played', -0.13873034119158478),\n",
       "   ('game', -0.1343606574253226)]),\n",
       " (3,\n",
       "  [('season', -0.3079965935097733),\n",
       "   ('state', 0.2837449877299943),\n",
       "   ('league', -0.2828300871373374),\n",
       "   (\"'s\", 0.24167997174164133),\n",
       "   ('team', -0.2405771822317494),\n",
       "   ('football', -0.20018747013350344),\n",
       "   ('played', -0.17474895838590396),\n",
       "   ('club', -0.15859749877178025),\n",
       "   ('game', -0.13620228562693282),\n",
       "   ('school', 0.1306806265573009)]),\n",
       " (4,\n",
       "  [('album', -0.4494565856550878),\n",
       "   (\"'s\", -0.44025879949257474),\n",
       "   (\"''\", 0.28470778635580757),\n",
       "   ('released', -0.2356708424412762),\n",
       "   ('state', 0.19574650075024327),\n",
       "   ('single', -0.18148183071394094),\n",
       "   ('band', -0.15409085100277084),\n",
       "   ('league', 0.14441373477118546),\n",
       "   ('football', 0.1256101693236032),\n",
       "   ('school', 0.1214514777839529)]),\n",
       " (5,\n",
       "  [('th', 0.615746043605832),\n",
       "   ('division', 0.44152203193051),\n",
       "   ('regiment', 0.3226858349518607),\n",
       "   ('infantry', 0.26578507588943284),\n",
       "   ('army', 0.21107119238167546),\n",
       "   ('rd', 0.11142968253379643),\n",
       "   ('war', 0.1098895323143517),\n",
       "   ('school', -0.10884192916492694),\n",
       "   ('renamed', 0.09121076948815049),\n",
       "   ('state', -0.09112980300212073)]),\n",
       " (6,\n",
       "  [('lake', -0.32537481474168345),\n",
       "   ('river', -0.3220804768826288),\n",
       "   ('party', 0.2785490532647128),\n",
       "   ('album', -0.21350973569165838),\n",
       "   (\"'s\", 0.2072079480871495),\n",
       "   ('bridge', -0.18591268333230385),\n",
       "   ('election', 0.1781573636429266),\n",
       "   ('line', -0.154004461626619),\n",
       "   ('station', -0.15185329279360407),\n",
       "   ('league', -0.12456778203876627)])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TOPICS = 7\n",
    "\n",
    "model = LsiModel(corpus=document_term_matrix, num_topics=NUM_TOPICS, id2word=term_dictionary)\n",
    "\n",
    "# To show the keywords associated with the topics, along with the score\n",
    "lsi_topics = model.show_topics(num_topics=NUM_TOPICS, formatted=False)\n",
    "lsi_topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d8db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
