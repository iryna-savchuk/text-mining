{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be00da1",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a47b8",
   "metadata": {},
   "source": [
    "In this part we will cover Transformer based large pretrained models. \n",
    "\n",
    "This notebook focus on showing how you can use the widely known Hugging Face library to apply different types of transformer models to a different range of tasks.\n",
    "\n",
    "We hope you learn how you can levarage pretrained transformer-based models and how to fine-tune them to a specific downstream task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0a3ca",
   "metadata": {},
   "source": [
    "To dive deep into transformers, we recommend to start by reading\n",
    "    \n",
    "- http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "This gives a step by step explanation of the original paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "![4](https://lilianweng.github.io/lil-log/assets/images/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f3541",
   "metadata": {},
   "source": [
    "#### Code implemantation\n",
    "\n",
    "It's also good pratice to try implementing yourself the original code before using a Transformers library:\n",
    "\n",
    "Code Implementation\n",
    "- http://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cca0df",
   "metadata": {},
   "source": [
    "# Hugging Face ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500226a2",
   "metadata": {},
   "source": [
    "\"Training a transformer model and deploying these models can be quite challeging. In general these models have millions to tens of billions of parameters and requires large amount of data. \n",
    "\n",
    "This becomes very costly in terms of time and compute resources. It even translates to environmental impact. Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs!\n",
    "\n",
    "\n",
    "The Transformers library was created to solve this problem. Its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved. \n",
    "\n",
    "The Hugging Face Transformers library provides the functionality to create and use those shared models.\"\n",
    "\n",
    "For more details look at the official [Hugging Face course](https://huggingface.co/course/chapter1/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd3962",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b71577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ad1dfc82254baa90884d9a55f02ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8213cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install ipywidgets --user\n",
    "!pip3 install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf1f14",
   "metadata": {},
   "source": [
    "### Important:\n",
    "\n",
    "After this instalation --> don't forget to restart the kernel of the jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a61b8b",
   "metadata": {},
   "source": [
    "## Examples of transformer architectures available\n",
    "\n",
    "As menioned above, the Transformer architecture was introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017, in which the focus of the original research was on translation tasks using encoder-decoder blocks. \n",
    "\n",
    "This was then followed by the introduction of several influential models, including encoder only models (e.g., BERT) and decoder only models (e.g., GPT), and there have been also a variety of encoder-decoder transformer based models (e.g., BART and T5). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c4f91",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572ab73",
   "metadata": {},
   "source": [
    "Encoder models use only the encoder block of a Transformer model. \n",
    "\n",
    "These models are often characterized as having ‚Äúbi-directional‚Äù attention, and are often called auto-encoding models.\n",
    "\n",
    "Encoder models are best suited for tasks requiring an understanding of the full sentence, such as:\n",
    "   - sentence classification\n",
    "   - named entity recognition (word classification in general), \n",
    "   - extractive question answering\n",
    "   - sentence representation (contextual embeddings)\n",
    "\n",
    "    \n",
    "The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.\n",
    "\n",
    "\n",
    "There a variaty of encoder models available at Hugging Face. Some examples include:\n",
    "\n",
    "- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\n",
    "- [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569c62a",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Decoder models use only the decoder of a Transformer model. At each stage, for a given word, the self-attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.\n",
    "\n",
    "The pretraining of decoder models usually revolves around predicting the next word in the sentence.\n",
    "\n",
    "These models are best suited for tasks involving text generation.\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "Some examples of decoder models available in Hugging Face include:\n",
    "- [GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)\n",
    "- [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)\n",
    "- [Transformer XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b257cd",
   "metadata": {},
   "source": [
    "## Encoder-decoder models\n",
    "\n",
    "Encoder-decoder models (also called sequence-to-sequence models) use both parts of the Transformer architecture. \n",
    "\n",
    "Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input (conditional text generation), such as:\n",
    "- summarization\n",
    "- translation\n",
    "- or generative question answering\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "- [BART](https://huggingface.co/docs/transformers/model_doc/bart)\n",
    "- [Marian](https://huggingface.co/docs/transformers/model_doc/marian)\n",
    "- [T5](https://huggingface.co/docs/transformers/model_doc/t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea21065",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0df12a",
   "metadata": {},
   "source": [
    "The most basic object in the Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.\n",
    "\n",
    "See all details of pipeline here: https://huggingface.co/docs/transformers/v4.16.0/en/main_classes/pipelines#transformers.TranslationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc34054e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.21\n",
      "\n",
      "  Using cached numpy-1.21.0-cp38-cp38-macosx_10_9_x86_64.whl (16.9 MB)\n",
      "\n",
      "Installing collected packages: numpy\n",
      "\n",
      "  Attempting uninstall: numpy\n",
      "\n",
      "    Found existing installation: numpy 1.24.3\n",
      "\n",
      "    Uninstalling numpy-1.24.3:\n",
      "\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "\n",
      "torchvision 0.9.1 requires torch==1.8.1, but you have torch 1.9.0 which is incompatible.\n",
      "\n",
      "datawig 0.2.0 requires scikit-learn==1.0.2, but you have scikit-learn 1.1.1 which is incompatible.\u001b[0m\n",
      "\n",
      "Successfully installed numpy-1.21.0\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install numpy==1.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce0a1da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:02:02.049121Z",
     "iopub.status.busy": "2023-05-21T15:02:02.048715Z",
     "iopub.status.idle": "2023-05-21T15:02:05.646295Z",
     "shell.execute_reply": "2023-05-21T15:02:05.642257Z",
     "shell.execute_reply.started": "2023-05-21T15:02:02.049087Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b522aa09d84af18bf6eb6c0142cff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62045f5392f40f58530fd4c783050fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cffc6f6b22645a0b975e528c2d8d9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af6e6cdb6c24c7fbc03230c779883d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598050713539124},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\"I've been waiting for a HuggingFace course my whole life.\",\n",
    "            \"I hate this so much!\"])\n",
    "\n",
    "# By default, this pipeline selects a particular pretrained model \n",
    "# that has been fine-tuned for sentiment analysis in English. \n",
    "\n",
    "# The model is downloaded and cached when you create the classifier object. \n",
    "# If you rerun the command, the cached model will be used instead \n",
    "# and there is no need to download the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093a9bd",
   "metadata": {},
   "source": [
    "Try with your own text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9c90a1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:02:17.393893Z",
     "iopub.status.busy": "2023-05-21T15:02:17.393527Z",
     "iopub.status.idle": "2023-05-21T15:03:34.377838Z",
     "shell.execute_reply": "2023-05-21T15:03:34.376267Z",
     "shell.execute_reply.started": "2023-05-21T15:02:17.393864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Good soup!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998539686203003}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = input()\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892547fc",
   "metadata": {},
   "source": [
    "Besides sentiment analysis, some of the currently available pipelines are:\n",
    "\n",
    "- feature-extraction (get the vector representation of a text)\n",
    "- fill-mask\n",
    "- ner (named entity recognition)\n",
    "- question-answering\n",
    "- summarization\n",
    "- text-generation\n",
    "- translation\n",
    "- zero-shot-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3caf3",
   "metadata": {},
   "source": [
    "Let‚Äôs now for instance check how to use a pipeline to generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d20b71e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:03:39.570220Z",
     "iopub.status.busy": "2023-05-21T15:03:39.569832Z",
     "iopub.status.idle": "2023-05-21T15:03:48.623306Z",
     "shell.execute_reply": "2023-05-21T15:03:48.617335Z",
     "shell.execute_reply.started": "2023-05-21T15:03:39.570165Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90405de281ab40d89eb1ffb2426beaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550d52f932d74821a5535a056a0dcae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d207845ce5e84678a504819567b98616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c237297515214d22bceae7a0551e4bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7099377ef93c432fb258e110eeb9cfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265aeca622994f189bfc6ea50d5a0fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use transformer models and how to use them in your own projects.\\n\\nThe first thing you will learn is how to use transformer models in your own projects.\\n\\nThe second thing you will learn'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to use transformer models and\", do_sample=False)\n",
    "\n",
    "# if set to True, this parameter enables decoding strategies such as multinomial sampling, \n",
    "# beam-search multinomial sampling, Top-K sampling and Top-p sampling. \n",
    "# All these strategies select the next token from the probability distribution over the entire \n",
    "# vocabulary with various strategy-specific adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e4365",
   "metadata": {},
   "source": [
    "The previous examples used the default model for the task at hand, but you can also choose a particular model for any of the above tasks. \n",
    "\n",
    "For the specific case of the generation task, you can also control:\n",
    "- how many different sequences are generated with the argument num_return_sequences\n",
    "- the total length of the output text with the argument max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c500502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:06:00.086329Z",
     "iopub.status.busy": "2023-05-21T15:06:00.085912Z",
     "iopub.status.idle": "2023-05-21T15:06:15.704542Z",
     "shell.execute_reply": "2023-05-21T15:06:15.703528Z",
     "shell.execute_reply.started": "2023-05-21T15:06:00.086297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Today I feel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Today I feel so ashamed. I was never able to stop being angry at myself. I was never ever even able to fight for equality. I needed'},\n",
       " {'generated_text': 'Today I feel it may be the end of the year for me and my little family. It is not only fun to do so, but it really'},\n",
       " {'generated_text': 'Today I feel that the future of gaming is bright and a part of that. With the success of the past couple of times I wanted to see the'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    input(),\n",
    "    max_length=30,\n",
    "    num_return_sequences=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc5721a",
   "metadata": {},
   "source": [
    "Try it yourself with different pipelines available. \n",
    "\n",
    "To check how to use one in specific see the documentation: https://huggingface.co/docs/transformers/v4.16.0/en/main_classes/pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d091bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:07:02.663278Z",
     "iopub.status.busy": "2023-05-21T15:07:02.662871Z",
     "iopub.status.idle": "2023-05-21T15:07:13.828027Z",
     "shell.execute_reply": "2023-05-21T15:07:13.826917Z",
     "shell.execute_reply.started": "2023-05-21T15:07:02.663245Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24eb691f55b941eba34065fea636360f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17b78dedf1540bdb66649085d3342b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114f42259d8542169d6431fc83d94151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30efdca0aad54c89a0437c8c13385660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd46634bfe64cd18ec9045bc33487ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Un homme se prom√®ne dans un bar anglais.'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying the machine translation pipeline\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "translator(\"A man walks into a English bar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7951592",
   "metadata": {},
   "source": [
    "\n",
    "# Pipeline with a real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ce5e3",
   "metadata": {},
   "source": [
    "Lets try this pipeline with sentences from a dataset, such as the IMDB dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6efe74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:20:08.506525Z",
     "iopub.status.busy": "2023-05-21T15:20:08.505530Z",
     "iopub.status.idle": "2023-05-21T15:20:08.541113Z",
     "shell.execute_reply": "2023-05-21T15:20:08.539660Z",
     "shell.execute_reply.started": "2023-05-21T15:20:08.506488Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data, datasets\n",
    "\n",
    "TEXT = data.Field()\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "_, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae980a23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:16:35.282406Z",
     "iopub.status.busy": "2023-05-21T15:16:35.282079Z",
     "iopub.status.idle": "2023-05-21T15:16:35.322783Z",
     "shell.execute_reply": "2023-05-21T15:16:35.319750Z",
     "shell.execute_reply.started": "2023-05-21T15:16:35.282380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ideally we should run all the senteces to the model and see it's performance on test (or validation set).\n",
    "# But lets just pick first some number of sentences to run faster (or you could use GPU): such as first 50 sentences and last 50 sentences\n",
    "labels=list(test_data.label)[:50]+list(test_data.label)[-50:] \n",
    "sents_with_tokens=list(test_data.text)[:50]+list(test_data.text)[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecce6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to have the corresponding sentences (and not tokens) \n",
    "# so that the pipeline tokenizes the text by itself according to the model tokenization\n",
    "# we can give a max n¬∫ of words to be faster (e.g., in terms of performing self attention)\n",
    "MAX_LEN=500\n",
    "sents=[\" \".join(tokens)[:MAX_LEN] for tokens in sents_with_tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c5510b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:21<00:00,  4.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "preds=[]\n",
    "for sent in tqdm(sents):\n",
    "    preds.append(classifier(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10f81e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.78\n"
     ]
    }
   ],
   "source": [
    "count_correct=0\n",
    "for i in range(len(preds)):\n",
    "    if labels[i] in preds[i][0].get(\"label\").lower():\n",
    "        count_correct +=1\n",
    "print(\"acc\", count_correct/len(labels))              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ecaaa",
   "metadata": {},
   "source": [
    "# Dive deep into Hugging Face\n",
    "\n",
    "\n",
    "Besides the currently available pipelines, we can dive deep and use any model available in HG and apply it to any given task. \n",
    "\n",
    "We‚Äôll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions. \n",
    "\n",
    "\n",
    "## Behind pipeline \n",
    "Lets begin with an end-to-end example where we use a model and a tokenizer together to replicate the pipeline() function of sentiment analysis introduced before.\n",
    "\n",
    "There are three main steps involved when you pass some text to a pipeline:\n",
    "1. <b> Preprocessing with a tokenizer:</b> The text is preprocessed into a format the model can understand.\n",
    "2. <b> Going through the model:</b> The preprocessed inputs are passed to the model.\n",
    "3. <b> Postprocessing the output\n",
    ": </b> The predictions of the model are post-processed, so you can make sense of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d43b8",
   "metadata": {},
   "source": [
    "![0](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a5578",
   "metadata": {},
   "source": [
    "### 1. Preprocessing with a tokenizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb95681",
   "metadata": {},
   "source": [
    "Like other neural networks, Transformer models can‚Äôt process raw text directly.\n",
    "\n",
    "So the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. \n",
    "\n",
    "To do this we use a <b> tokenizer</b>. They serve the purpose to translate text into data that can be processed by the model.\n",
    "\n",
    "Tokenizer will be responsible for:\n",
    "\n",
    "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "- Mapping each token to an integer\n",
    "- Adding additional inputs that may be useful to the model (e.g., attention mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc3386",
   "metadata": {},
   "source": [
    "#### Define tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da41091",
   "metadata": {},
   "source": [
    "The tokenizer and the model should always be from the same checkpoint. Therefore, we need to define the tokenizer with the checkpoint name of the corresponding model. \n",
    "\n",
    "\n",
    "To do this, we use the <b> AutoTokenizer class </b> and its <b> from_pretrained() </b> method with the checkpoint name of the corresponding model inside it.\n",
    "\n",
    "This will automatically fetch the data associated with the model‚Äôs tokenizer (as the vocabulary) and cache it (so it‚Äôs only downloaded the first time you run the code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac1e1517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5057ffd4d7914755ad0afcb9c6184403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d09c8e941b4e74afad4e974b959b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658a53902c484aa5bd39acfc58140190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077db0b8c8d44081b21b60a4ca0dc823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a907a3",
   "metadata": {},
   "source": [
    "Alternatively to the wrapper AutoModel class, you can use directly the class of the corresponding tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c6b148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95f75e",
   "metadata": {},
   "source": [
    "You can use either one of the two. But note that the AutoTokenizer produces checkpoint-agnostic code\n",
    "\n",
    "- The AutoTokenizer will works for other checkpoints besides BERT (e.g.: distilbert-base-uncased-finetuned-sst-2-english), \n",
    "- Whereas the BertTokenizer just works for checkpoints related to BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d39d75aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "\n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "\n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b130e",
   "metadata": {},
   "source": [
    "To mimic our sentiment analysis pipeline, let's actually use the DistilBERT model and thus use the corresponding DistilBERT tokenizer (with checkpoint \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d6434cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650b1e6",
   "metadata": {},
   "source": [
    "#### Encode the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0aa0f4",
   "metadata": {},
   "source": [
    "Once we have the tokenizer, we can directly pass our sentences to it.\n",
    "\n",
    "Translating text to numbers is known as encoding. Encoding is done in a two-step process: \n",
    "- the tokenization\n",
    "- followed by the conversion to input IDs.\n",
    "\n",
    "---------\n",
    "To know more about each of the 2 steps:\n",
    "The first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called tokens. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\n",
    "\n",
    "The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method. Again, we need to use the same vocabulary used when the model was pretrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbfd46cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "\n",
      "        [  101,  1045,  2424,  2023,  2200,  3733,   999,   102,     0,     0,\n",
      "\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "    \"I find this very easy!\"\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    raw_inputs,\n",
    "    padding=True,        # since the sentences might not have the same size, don't forget to padding. \n",
    "    return_tensors=\"pt\"  # to return with pytorch tensors\n",
    ")\n",
    "\n",
    "# - Feeding your raw_sentences to the tokenizer will give the corresponding input_id\n",
    "# - As well as the attention_mask, usufel so that there is no self attention between padding words. \n",
    "\n",
    "# - In case of BERT it will also produce \"token_type_ids\" \n",
    "#   useful when the model receives sentence A and B together (e.g., for text similarity tasks) \n",
    "      # (0 corresponds to a sentence A token,\n",
    "      # 1 corresponds to a sentence B token.)\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443ce34",
   "metadata": {},
   "source": [
    "Note that each word can correspond to more than one id, since DistilBERT uses subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f48799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(raw_inputs[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd95557",
   "metadata": {},
   "source": [
    "Besides encoding the corresponding input text, we can also decode, going the other way around: from vocabulary indices to get the corresponding string.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dd74d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens id: [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "\n",
      "\n",
      "\n",
      "Decoded inputs: [CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Without special tokens: i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "tokens_id = tokenizer.encode(raw_inputs[0])\n",
    "print(\"Tokens id:\", tokens_id)\n",
    "\n",
    "decode_inputs = tokenizer.decode(tokens_id)\n",
    "print(\"\\nDecoded inputs:\", decode_inputs)\n",
    "\n",
    "decode_inputs = tokenizer.decode(tokens_id, skip_special_tokens=True)\n",
    "print(\"\\nWithout special tokens:\", decode_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f5fef",
   "metadata": {},
   "source": [
    "# Going through the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5fd63",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d2374",
   "metadata": {},
   "source": [
    "We can download our model that is already trained in the same way that we did with our tokenizer:\n",
    "\n",
    "- You can use AutoModel class which also has from_pretrained() method or use directly\n",
    "- You could replace AutoModel directly with the corresponding model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fc9ebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# we have downloaded the same checkpoint we used in our pipeline before\n",
    "# threfore it should actually have been cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e677b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertModel\n",
    "\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76348c8e",
   "metadata": {},
   "source": [
    "It's also possible to load a model from scratch without trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "406486b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig\n",
    "\n",
    "config = DistilBertConfig()\n",
    "model_without_pretrained = DistilBertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2525ee48",
   "metadata": {},
   "source": [
    "#### Feed the inputs\n",
    "\n",
    "This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.\n",
    "\n",
    "We can now feed the inputs we preprocessed before (with the tokenizer) to our model:\n",
    "    \n",
    "    {'input_ids': \n",
    "    \n",
    "        tensor([\n",
    "            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
    "              2607,  2026,  2878,  2166,  1012,   102],\n",
    "            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
    "                 0,     0,     0,     0,     0,     0],\n",
    "            [  101,  1045,  2424,  2023,  2200,  3733,   999,   102,     0,     0,\n",
    "                 0,     0,     0,     0,     0,     0]]),\n",
    "             \n",
    "     'attention_mask': \n",
    "     \n",
    "         tensor(\n",
    "            [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a22b0378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
      "\n",
      "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
      "\n",
      "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
      "\n",
      "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
      "\n",
      "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
      "\n",
      "\n",
      "\n",
      "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
      "\n",
      "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
      "\n",
      "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
      "\n",
      "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
      "\n",
      "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]],\n",
      "\n",
      "\n",
      "\n",
      "        [[-0.3841, -0.1072,  0.3243,  ...,  0.2156,  0.2593,  0.0866],\n",
      "\n",
      "         [ 0.3034,  0.3026,  0.2005,  ...,  0.4373,  0.5169, -0.0845],\n",
      "\n",
      "         [-0.4797,  0.1210,  0.5126,  ...,  0.1014,  0.4086,  0.2696],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [-0.3598, -0.0655,  0.4618,  ...,  0.3586,  0.2877,  0.0428],\n",
      "\n",
      "         [-0.3949, -0.0937,  0.4477,  ...,  0.3372,  0.2124,  0.0469],\n",
      "\n",
      "         [-0.4066, -0.0487,  0.4370,  ...,  0.3481,  0.2206,  0.1476]]],\n",
      "\n",
      "       grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs) # you give further arguments: output_attentions=True and output_hidden_states:True\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccfac4c",
   "metadata": {},
   "source": [
    "Note that the outputs of Transformers models behave like named tuples or dictionaries. \n",
    "\n",
    "You can access the elements by:\n",
    "- attributes (like we did) \n",
    "- or by key (outputs[\"last_hidden_state\"])\n",
    "- or even by index if you know exactly where the thing you are looking for is (outputs[0]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4cf25d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
      "\n",
      "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
      "\n",
      "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
      "\n",
      "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
      "\n",
      "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
      "\n",
      "\n",
      "\n",
      "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
      "\n",
      "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
      "\n",
      "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
      "\n",
      "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
      "\n",
      "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]],\n",
      "\n",
      "\n",
      "\n",
      "        [[-0.3841, -0.1072,  0.3243,  ...,  0.2156,  0.2593,  0.0866],\n",
      "\n",
      "         [ 0.3034,  0.3026,  0.2005,  ...,  0.4373,  0.5169, -0.0845],\n",
      "\n",
      "         [-0.4797,  0.1210,  0.5126,  ...,  0.1014,  0.4086,  0.2696],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [-0.3598, -0.0655,  0.4618,  ...,  0.3586,  0.2877,  0.0428],\n",
      "\n",
      "         [-0.3949, -0.0937,  0.4477,  ...,  0.3372,  0.2124,  0.0469],\n",
      "\n",
      "         [-0.4066, -0.0487,  0.4370,  ...,  0.3481,  0.2206,  0.1476]]],\n",
      "\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"last hidden state\", outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e3f47",
   "metadata": {},
   "source": [
    "Note also that given the input to the model, it will output a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
    "\n",
    "The vector output by the Transformer module generally has three dimensions:\n",
    "\n",
    "- Batch size: The number of sequences processed at a time (2 in our example).\n",
    "- Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "- Hidden size: The vector dimension of each model input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2dca71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size torch.Size([3, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"size\", outputs.last_hidden_state.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e373d",
   "metadata": {},
   "source": [
    "## Postprocessing the output\n",
    " \n",
    "While the hidden states that were outputed can be useful on their own.\n",
    "\n",
    "\n",
    "In this case for sentiment analysis we are more interesting in using the corresponding classification head. \n",
    "\n",
    "For our example, we will use the DistilBERT model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won‚Äôt actually use the DistilBertModel class but the DistilBertForSequenceClassification (or AutoModelForSequenceClassification class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0060cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "    \"I find this very easy!\"\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    raw_inputs,\n",
    "    padding=True,        # since the sentences might not have the same size, don't forget to padding. \n",
    "    return_tensors=\"pt\"  # to return with pytorch tensors\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e866f254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5607,  1.6123],\n",
       "        [ 4.1692, -3.3464],\n",
       "        [-3.0649,  3.0434]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fbc54",
   "metadata": {},
   "source": [
    "Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. \n",
    "\n",
    "To be converted to probabilities, they need to go through a SoftMax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01e83cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "\n",
      "        [9.9946e-01, 5.4418e-04],\n",
      "\n",
      "        [2.2193e-03, 9.9778e-01]])\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.nn.functional.softmax(logits, dim=-1) # A dimension along which Softmax will be computed (so every slice along dim will sum to 1).\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a945a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been waiting for a HuggingFace course my whole life. POSITIVE\n",
      "\n",
      "I hate this so much! NEGATIVE\n",
      "\n",
      "I find this very easy! POSITIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_inputs\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i][1].item()>=0.5:\n",
    "        print(raw_inputs[i],model.config.id2label[1])\n",
    "    else:\n",
    "        print(raw_inputs[i],model.config.id2label[0])\n",
    "\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b3253",
   "metadata": {},
   "source": [
    "We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0dd54",
   "metadata": {},
   "source": [
    "# Fine-Tunings Transformers for Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebeaec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T11:57:15.043611Z",
     "iopub.status.busy": "2023-05-21T11:57:15.043113Z",
     "iopub.status.idle": "2023-05-21T11:57:26.372104Z",
     "shell.execute_reply": "2023-05-21T11:57:26.370643Z",
     "shell.execute_reply.started": "2023-05-21T11:57:15.043571Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.28.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6253a0c",
   "metadata": {},
   "source": [
    "Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework for returning some output from an input, like translation or summarization. Translation systems are commonly used for translation between different language texts, but it can also be used for speech or some combination in between like text-to-speech or speech-to-text.\n",
    "\n",
    "![3](https://raw.githubusercontent.com/huggingface/notebooks/c5d94e54a771af91c6732a5313c49c2c42ac5cff/examples/images/translation.png)\n",
    "\n",
    "This guide will show you how to:\n",
    "\n",
    "1. Finetune [T5](https://huggingface.co/t5-small) on the English-French subset of the [OPUS Books](https://huggingface.co/datasets/opus_books) dataset to translate English text to French.\n",
    "2. Use your finetuned model for inference.\n",
    "\n",
    "<Tip>\n",
    "The task illustrated in this tutorial is supported by the following model architectures:\n",
    "\n",
    "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
    "\n",
    "[BART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bart), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bigbird_pegasus), [Blenderbot](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/blenderbot), [BlenderbotSmall](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/blenderbot-small), [Encoder decoder](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/encoder-decoder), [FairSeq Machine-Translation](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fsmt), [GPTSAN-japanese](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gptsan-japanese), [LED](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/led), [LongT5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longt5), [M2M100](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/m2m_100), [Marian](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/marian), [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart), [MT5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mt5), [MVP](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mvp), [NLLB](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nllb), [NLLB-MOE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nllb-moe), [Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/pegasus), [PEGASUS-X](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/pegasus_x), [PLBart](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/plbart), [ProphetNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/prophetnet), [SwitchTransformers](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/switch_transformers), [T5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/t5), [XLM-ProphetNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-prophetnet)\n",
    "\n",
    "<!--End of the generated tip-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d76de",
   "metadata": {},
   "source": [
    "## Load OPUS Books dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fb6f3",
   "metadata": {},
   "source": [
    "Start by loading the English-French subset of the [OPUS Books](https://huggingface.co/datasets/opus_books) dataset from the Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "047d009a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:04:42.889143Z",
     "iopub.status.busy": "2023-05-21T14:04:42.888786Z",
     "iopub.status.idle": "2023-05-21T14:04:52.466253Z",
     "shell.execute_reply": "2023-05-21T14:04:52.465264Z",
     "shell.execute_reply.started": "2023-05-21T14:04:42.889112Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1fc46e76d749c88edf8d0d495cd94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d92a7b16644dceb012b25d130e7916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/7.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset opus_books/en-fr (download: 11.45 MiB, generated: 31.47 MiB, post-processed: Unknown size, total: 42.92 MiB) to /root/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3201d9c59b498ca76367436f17cc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset opus_books downloaded and prepared to /root/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78096a66717e4f48860bc0d7961d34d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "books = load_dataset(\"opus_books\", \"en-fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a364b",
   "metadata": {},
   "source": [
    "Split the dataset into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1f2eebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:04:52.469264Z",
     "iopub.status.busy": "2023-05-21T14:04:52.468283Z",
     "iopub.status.idle": "2023-05-21T14:04:52.588059Z",
     "shell.execute_reply": "2023-05-21T14:04:52.587150Z",
     "shell.execute_reply.started": "2023-05-21T14:04:52.469226Z"
    }
   },
   "outputs": [],
   "source": [
    "books = books[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85afe8",
   "metadata": {},
   "source": [
    "Then take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c6d6c25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:04:52.591984Z",
     "iopub.status.busy": "2023-05-21T14:04:52.589583Z",
     "iopub.status.idle": "2023-05-21T14:04:52.601148Z",
     "shell.execute_reply": "2023-05-21T14:04:52.600316Z",
     "shell.execute_reply.started": "2023-05-21T14:04:52.591949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6091',\n",
       " 'translation': {'en': 'What a stroke was this for poor Jane! who would willingly have gone through the world without believing that so much wickedness existed in the whole race of mankind, as was here collected in one individual.',\n",
       "  'fr': 'Quel coup pour la pauvre Jane qui aurait parcouru le monde entier sans s‚Äôimaginer qu‚Äôil exist√¢t dans toute l‚Äôhumanit√© autant de noirceur qu‚Äôelle en d√©couvrait en ce moment dans un seul homme !'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299dce92",
   "metadata": {},
   "source": [
    "`translation`: an English and French translation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8316e3",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aeff72",
   "metadata": {},
   "source": [
    "The next step is to load a T5 tokenizer to process the English-French language pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df460a30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:04:52.604668Z",
     "iopub.status.busy": "2023-05-21T14:04:52.603962Z",
     "iopub.status.idle": "2023-05-21T14:04:54.487499Z",
     "shell.execute_reply": "2023-05-21T14:04:54.486432Z",
     "shell.execute_reply.started": "2023-05-21T14:04:52.604607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de4137d70d840d08e1a67fec62be808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4d8f60aa6340ce8479b905d2be874e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc1d47e2c81438789c99dbbde720f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2eb976",
   "metadata": {},
   "source": [
    "The preprocessing function you want to create needs to:\n",
    "\n",
    "1. Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
    "2. Tokenize the input (English) and target (French) separately because you can't tokenize French text with a tokenizer pretrained on an English vocabulary.\n",
    "3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45ca19e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:04:54.489417Z",
     "iopub.status.busy": "2023-05-21T14:04:54.488841Z",
     "iopub.status.idle": "2023-05-21T14:04:54.505187Z",
     "shell.execute_reply": "2023-05-21T14:04:54.504158Z",
     "shell.execute_reply.started": "2023-05-21T14:04:54.489379Z"
    }
   },
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be053a",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "feba4f47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:04:54.507648Z",
     "iopub.status.busy": "2023-05-21T14:04:54.506987Z",
     "iopub.status.idle": "2023-05-21T14:05:27.632840Z",
     "shell.execute_reply": "2023-05-21T14:05:27.631829Z",
     "shell.execute_reply.started": "2023-05-21T14:04:54.507615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb29e192f36c419a84b89eeefe13e634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376a158122ce4fc1a8bbf52facb009d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a716ad",
   "metadata": {},
   "source": [
    "Now create a batch of examples using [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd9eee1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:05:27.637965Z",
     "iopub.status.busy": "2023-05-21T14:05:27.637328Z",
     "iopub.status.idle": "2023-05-21T14:05:27.647596Z",
     "shell.execute_reply": "2023-05-21T14:05:27.646140Z",
     "shell.execute_reply.started": "2023-05-21T14:05:27.637927Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint) # padding - default is True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81cbbd",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69fe963",
   "metadata": {},
   "source": [
    "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) metric (see the Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fdc250d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:05:40.395728Z",
     "iopub.status.busy": "2023-05-21T14:05:40.395379Z",
     "iopub.status.idle": "2023-05-21T14:05:53.582575Z",
     "shell.execute_reply": "2023-05-21T14:05:53.581392Z",
     "shell.execute_reply.started": "2023-05-21T14:05:40.395698Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.28.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.0)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-2.7.0 sacrebleu-2.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers datasets evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "997f30b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:05:53.586359Z",
     "iopub.status.busy": "2023-05-21T14:05:53.585522Z",
     "iopub.status.idle": "2023-05-21T14:05:54.263032Z",
     "shell.execute_reply": "2023-05-21T14:05:54.261945Z",
     "shell.execute_reply.started": "2023-05-21T14:05:53.586318Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\") # sacrebleu - to calculate Bleu that is a metric used in translations tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8482a3",
   "metadata": {},
   "source": [
    "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the SacreBLEU score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a836ceb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:05:54.266814Z",
     "iopub.status.busy": "2023-05-21T14:05:54.264865Z",
     "iopub.status.idle": "2023-05-21T14:05:54.278734Z",
     "shell.execute_reply": "2023-05-21T14:05:54.277268Z",
     "shell.execute_reply.started": "2023-05-21T14:05:54.266776Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de515895",
   "metadata": {},
   "source": [
    "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56e1c3",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1cc6d7",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "You're ready to start training your model now! Load T5 with [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "407b4ae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:05:54.282133Z",
     "iopub.status.busy": "2023-05-21T14:05:54.281177Z",
     "iopub.status.idle": "2023-05-21T14:05:57.412427Z",
     "shell.execute_reply": "2023-05-21T14:05:57.411342Z",
     "shell.execute_reply.started": "2023-05-21T14:05:54.282098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0264ed233a4f472584d64edd23c50183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37caf78309a44a42b760ca0b9a2cd67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1a6d4660414704b25d97180986fa0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065d567",
   "metadata": {},
   "source": [
    "At this point, only three steps remain:\n",
    "\n",
    "1. Define your training hyperparameters in [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the SacreBLEU metric and save the training checkpoint.\n",
    "2. Pass the training arguments to [Seq2SeqTrainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
    "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "939e0255",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:05:57.415299Z",
     "iopub.status.busy": "2023-05-21T14:05:57.414446Z",
     "iopub.status.idle": "2023-05-21T14:59:42.038284Z",
     "shell.execute_reply": "2023-05-21T14:59:42.037278Z",
     "shell.execute_reply.started": "2023-05-21T14:05:57.415260Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/irodrigues/my_awesome_opus_books_model into local empty directory.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12710' max='12710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12710/12710 53:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.860300</td>\n",
       "      <td>1.628459</td>\n",
       "      <td>5.452700</td>\n",
       "      <td>17.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.807300</td>\n",
       "      <td>1.605240</td>\n",
       "      <td>5.639000</td>\n",
       "      <td>17.626200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12710, training_loss=1.8756247459669735, metrics={'train_runtime': 3219.5736, 'train_samples_per_second': 63.156, 'train_steps_per_second': 3.948, 'total_flos': 5000491514068992.0, 'train_loss': 1.8756247459669735, 'epoch': 2.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_opus_books_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, # if we have a GPU available, if not - simply comment this line\n",
    "    push_to_hub=True,  \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_books[\"train\"],\n",
    "    eval_dataset=tokenized_books[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6af3e1b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:59:42.040652Z",
     "iopub.status.busy": "2023-05-21T14:59:42.040023Z",
     "iopub.status.idle": "2023-05-21T15:00:28.566268Z",
     "shell.execute_reply": "2023-05-21T15:00:28.565249Z",
     "shell.execute_reply.started": "2023-05-21T14:59:42.040617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f84b77d103f4f2baaff887afabc121f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e08b7963a54897a71653aa79f547f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/May21_14-05-57_f1b1c7fb942b/events.out.tfevents.1684677962.f1b1c7fb942b.31.2:   0%|          ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/irodrigues/my_awesome_opus_books_model\n",
      "   ec9176c..f483b0c  main -> main\n",
      "\n",
      "To https://huggingface.co/irodrigues/my_awesome_opus_books_model\n",
      "   f483b0c..b7207cf  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/irodrigues/my_awesome_opus_books_model/commit/f483b0c90b7f945f6c14bbd26d8870ad114774ba'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b0be9",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "For a more in-depth example of how to finetune a model for translation, take a look at the corresponding\n",
    "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)\n",
    "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eee3a5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3d507",
   "metadata": {},
   "source": [
    "Great, now that you've finetuned a model, you can use it for inference!\n",
    "\n",
    "Come up with some text you'd like to translate to another language. For T5, you need to prefix your input depending on the task you're working on. For translation from English to French, you should prefix your input as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40b9e160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:00:33.697845Z",
     "iopub.status.busy": "2023-05-21T15:00:33.697464Z",
     "iopub.status.idle": "2023-05-21T15:00:33.706416Z",
     "shell.execute_reply": "2023-05-21T15:00:33.704223Z",
     "shell.execute_reply.started": "2023-05-21T15:00:33.697814Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17207fb1",
   "metadata": {},
   "source": [
    "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for translation with your model, and pass your text to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7fbd96b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:00:34.593378Z",
     "iopub.status.busy": "2023-05-21T15:00:34.592989Z",
     "iopub.status.idle": "2023-05-21T15:00:46.973856Z",
     "shell.execute_reply": "2023-05-21T15:00:46.972847Z",
     "shell.execute_reply.started": "2023-05-21T15:00:34.593346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e0c86c7399455f932ed81d526e3da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6c20bc8eb8494f99169c58b039ea44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8f32061b0c41229f964462899b8b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)neration_config.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7e1634df394a34808930cea730363f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2e600d550f495998fd5abcd25e5353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c7daee56d94da8bf3c6849aec3ebd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab87e7d9e304763830cae362b87e7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:958: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Legumes teilen Ressourcen mit Stickstoff-fixierenden Bakterien.'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"irodrigues/my_awesome_opus_books_model\")\n",
    "translator(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff103e2",
   "metadata": {},
   "source": [
    "You can also manually replicate the results of the `pipeline` if you'd like:\n",
    "\n",
    "Tokenize the text and return the `input_ids` as PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef08696a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:00:50.464060Z",
     "iopub.status.busy": "2023-05-21T15:00:50.463690Z",
     "iopub.status.idle": "2023-05-21T15:00:50.664510Z",
     "shell.execute_reply": "2023-05-21T15:00:50.663402Z",
     "shell.execute_reply.started": "2023-05-21T15:00:50.464029Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"irodrigues/my_awesome_opus_books_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6cacd",
   "metadata": {},
   "source": [
    "Use the [generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to create the translation. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text Generation](https://huggingface.co/docs/transformers/main/en/tasks/../main_classes/text_generation) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b094dd25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:00:52.838423Z",
     "iopub.status.busy": "2023-05-21T15:00:52.838026Z",
     "iopub.status.idle": "2023-05-21T15:00:55.044128Z",
     "shell.execute_reply": "2023-05-21T15:00:55.042942Z",
     "shell.execute_reply.started": "2023-05-21T15:00:52.838390Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"irodrigues/my_awesome_opus_books_model\")\n",
    "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811cf8d",
   "metadata": {},
   "source": [
    "Decode the generated token ids back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c64305e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T15:00:55.046482Z",
     "iopub.status.busy": "2023-05-21T15:00:55.045874Z",
     "iopub.status.idle": "2023-05-21T15:00:55.058878Z",
     "shell.execute_reply": "2023-05-21T15:00:55.057690Z",
     "shell.execute_reply.started": "2023-05-21T15:00:55.046443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Les l√©gumes partagent les ressources avec des bact√©ries fixantes d'azote.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e30b3",
   "metadata": {},
   "source": [
    "# Fine-Tuning Transformers for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8f6c7",
   "metadata": {},
   "source": [
    "Text classification is a common NLP task that assigns a label or class to text. Some of the largest companies run text classification in production for a wide range of practical applications. One of the most popular forms of text classification is sentiment analysis, which assigns a label like üôÇ positive, üôÅ negative, or üòê neutral to a sequence of text.\n",
    "\n",
    "![1](https://github.com/huggingface/notebooks/blob/main/examples/images/text_classification.png?raw=1)\n",
    "\n",
    "\n",
    "This guide will show you how to:\n",
    "\n",
    "1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [IMDb](https://huggingface.co/datasets/imdb) dataset to determine whether a movie review is positive or negative.\n",
    "2. Use your finetuned model for inference.\n",
    "\n",
    "<Tip>\n",
    "The task illustrated in this tutorial is supported by the following model architectures:\n",
    "\n",
    "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
    "\n",
    "[ALBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/albert), [BART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bart), [BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bert), [BigBird](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/big_bird), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bigbird_pegasus), [BioGpt](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/biogpt), [BLOOM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bloom), [CamemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/camembert), [CANINE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/canine), [ConvBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/convbert), [CTRL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ctrl), [Data2VecText](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/data2vec-text), [DeBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta), [DeBERTa-v2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta-v2), [DistilBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/distilbert), [ELECTRA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/electra), [ERNIE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie), [ErnieM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie_m), [ESM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/esm), [FlauBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/flaubert), [FNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fnet), [Funnel Transformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/funnel), [GPT-Sw3](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt-sw3), [OpenAI GPT-2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt2), [GPTBigCode](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_bigcode), [GPT Neo](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neo), [GPT NeoX](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neox), [GPT-J](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gptj), [I-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ibert), [LayoutLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlm), [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv2), [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv3), [LED](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/led), [LiLT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/lilt), [LLaMA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/llama), [Longformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longformer), [LUKE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/luke), [MarkupLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/markuplm), [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart), [MEGA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mega), [Megatron-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/megatron-bert), [MobileBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mobilebert), [MPNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mpnet), [MVP](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mvp), [Nezha](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nezha), [Nystr√∂mformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nystromformer), [OpenLlama](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/open-llama), [OpenAI GPT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/openai-gpt), [OPT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/opt), [Perceiver](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/perceiver), [PLBart](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/plbart), [QDQBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/qdqbert), [Reformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/reformer), [RemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/rembert), [RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta), [RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta-prelayernorm), [RoCBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roc_bert), [RoFormer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roformer), [SqueezeBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/squeezebert), [TAPAS](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/tapas), [Transformer-XL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/transfo-xl), [XLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm), [XLM-RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta), [XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta-xl), [XLNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlnet), [X-MOD](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xmod), [YOSO](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/yoso)\n",
    "\n",
    "\n",
    "\n",
    "<!--End of the generated tip-->\n",
    "\n",
    "</Tip>\n",
    "\n",
    "Before you begin, make sure you have all the necessary libraries installed:\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets evaluate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c53a4",
   "metadata": {},
   "source": [
    "## Load IMDb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5f37b",
   "metadata": {},
   "source": [
    "Start by loading the IMDb dataset from the Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620d2247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:25:05.301118Z",
     "iopub.status.busy": "2023-05-21T13:25:05.300026Z",
     "iopub.status.idle": "2023-05-21T13:25:55.294636Z",
     "shell.execute_reply": "2023-05-21T13:25:55.293649Z",
     "shell.execute_reply.started": "2023-05-21T13:25:05.301073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4e60298ad54d649c7f35ed83d6db9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a9abb370a444519e87125c51293644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b98a65d70a4d75b8da1a209a339801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1468482eddcb471a99b7cf1d464a6d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69cb61",
   "metadata": {},
   "source": [
    "Then take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8b9921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:25:55.297671Z",
     "iopub.status.busy": "2023-05-21T13:25:55.296754Z",
     "iopub.status.idle": "2023-05-21T13:25:55.306498Z",
     "shell.execute_reply": "2023-05-21T13:25:55.305273Z",
     "shell.execute_reply.started": "2023-05-21T13:25:55.297631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74fd33",
   "metadata": {},
   "source": [
    "There are two fields in this dataset:\n",
    "\n",
    "- `text`: the movie review text.\n",
    "- `label`: a value that is either `0` for a negative review or `1` for a positive review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e017df",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef645c8",
   "metadata": {},
   "source": [
    "The next step is to load a DistilBERT tokenizer to preprocess the `text` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312cb54f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:25:55.308789Z",
     "iopub.status.busy": "2023-05-21T13:25:55.308226Z",
     "iopub.status.idle": "2023-05-21T13:25:58.562988Z",
     "shell.execute_reply": "2023-05-21T13:25:58.562021Z",
     "shell.execute_reply.started": "2023-05-21T13:25:55.308754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf55bfd6f9a94240b0026e2bb2b73056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb492b88851542ecb35cdbf52e6998fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf63ab9a9622441dbbe7a5a6b8435f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75952ab34ad4d7ca5b2a51707a37621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9bd36",
   "metadata": {},
   "source": [
    "Create a preprocessing fucnction to tokenize `text` and truncate sequences to be no longer than DistilBERT's maximum input length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e181af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:25:58.566630Z",
     "iopub.status.busy": "2023-05-21T13:25:58.565869Z",
     "iopub.status.idle": "2023-05-21T13:25:58.574631Z",
     "shell.execute_reply": "2023-05-21T13:25:58.570993Z",
     "shell.execute_reply.started": "2023-05-21T13:25:58.566593Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e7dc1",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1746091c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:25:58.576662Z",
     "iopub.status.busy": "2023-05-21T13:25:58.576250Z",
     "iopub.status.idle": "2023-05-21T13:27:18.122373Z",
     "shell.execute_reply": "2023-05-21T13:27:18.121264Z",
     "shell.execute_reply.started": "2023-05-21T13:25:58.576625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79c02c0c9ee4ccbaa4fddf8aaef2a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60756420b1cf43a2ac4d2175c24a07ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e81af760db439cb2ccbe7e70630c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2e0d9",
   "metadata": {},
   "source": [
    "Now create a batch of examples using [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6c4c6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:27:18.124830Z",
     "iopub.status.busy": "2023-05-21T13:27:18.124087Z",
     "iopub.status.idle": "2023-05-21T13:27:27.063389Z",
     "shell.execute_reply": "2023-05-21T13:27:27.062315Z",
     "shell.execute_reply.started": "2023-05-21T13:27:18.124793Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72740519",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976bb6c8",
   "metadata": {},
   "source": [
    "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3228a8af",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-21T13:27:48.608185Z",
     "iopub.status.busy": "2023-05-21T13:27:48.607740Z",
     "iopub.status.idle": "2023-05-21T13:28:01.555151Z",
     "shell.execute_reply": "2023-05-21T13:28:01.553894Z",
     "shell.execute_reply.started": "2023-05-21T13:27:48.608146Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.13.4)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.4.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.11.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5fb98b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:28:01.558112Z",
     "iopub.status.busy": "2023-05-21T13:28:01.557712Z",
     "iopub.status.idle": "2023-05-21T13:28:03.964760Z",
     "shell.execute_reply": "2023-05-21T13:28:03.963773Z",
     "shell.execute_reply.started": "2023-05-21T13:28:01.558074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce760068a9d4dc4a3e4957c9e26c2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00a62d",
   "metadata": {},
   "source": [
    "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd911866",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:28:03.966569Z",
     "iopub.status.busy": "2023-05-21T13:28:03.966147Z",
     "iopub.status.idle": "2023-05-21T13:28:03.973924Z",
     "shell.execute_reply": "2023-05-21T13:28:03.972437Z",
     "shell.execute_reply.started": "2023-05-21T13:28:03.966533Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dcd4a1",
   "metadata": {},
   "source": [
    "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849d3e1",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b772e2",
   "metadata": {},
   "source": [
    "Before you start training your model, create a map of the expected ids to their labels with `id2label` and `label2id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69716550",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:28:08.952294Z",
     "iopub.status.busy": "2023-05-21T13:28:08.951909Z",
     "iopub.status.idle": "2023-05-21T13:28:08.959230Z",
     "shell.execute_reply": "2023-05-21T13:28:08.958271Z",
     "shell.execute_reply.started": "2023-05-21T13:28:08.952261Z"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a8afb",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "You're ready to start training your model now! Load DistilBERT with [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) along with the number of expected labels, and the label mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd69132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:28:09.547781Z",
     "iopub.status.busy": "2023-05-21T13:28:09.547398Z",
     "iopub.status.idle": "2023-05-21T13:28:12.543948Z",
     "shell.execute_reply": "2023-05-21T13:28:12.543070Z",
     "shell.execute_reply.started": "2023-05-21T13:28:09.547749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23aa5c216d304b1db79c2333c589fbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006fbc9",
   "metadata": {},
   "source": [
    "At this point, only three steps remain:\n",
    "\n",
    "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the accuracy and save the training checkpoint.\n",
    "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
    "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5f4891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T13:30:23.768435Z",
     "iopub.status.busy": "2023-05-21T13:30:23.768037Z",
     "iopub.status.idle": "2023-05-21T14:01:26.274754Z",
     "shell.execute_reply": "2023-05-21T14:01:26.273588Z",
     "shell.execute_reply.started": "2023-05-21T13:30:23.768405Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/irodrigues/my_awesome_model into local empty directory.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20230521_133042-6m8up6r5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/text-mining/huggingface/runs/6m8up6r5' target=\"_blank\">flowing-plasma-3</a></strong> to <a href='https://wandb.ai/text-mining/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/text-mining/huggingface' target=\"_blank\">https://wandb.ai/text-mining/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/text-mining/huggingface/runs/6m8up6r5' target=\"_blank\">https://wandb.ai/text-mining/huggingface/runs/6m8up6r5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3126/3126 30:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.231800</td>\n",
       "      <td>0.186235</td>\n",
       "      <td>0.928160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.241715</td>\n",
       "      <td>0.930760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3126, training_loss=0.20540331512861196, metrics={'train_runtime': 1852.6272, 'train_samples_per_second': 26.989, 'train_steps_per_second': 1.687, 'total_flos': 6561288258498624.0, 'train_loss': 0.20540331512861196, 'epoch': 2.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf83699d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:03:01.828024Z",
     "iopub.status.busy": "2023-05-21T14:03:01.827337Z",
     "iopub.status.idle": "2023-05-21T14:03:27.715131Z",
     "shell.execute_reply": "2023-05-21T14:03:27.714133Z",
     "shell.execute_reply.started": "2023-05-21T14:03:01.827987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11d6f140e294ac3b4fb379bd769ee03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/May21_13-30-23_f1b1c7fb942b/events.out.tfevents.1684675833.f1b1c7fb942b.31.0:   0%|          ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/irodrigues/my_awesome_model\n",
      "   e9a55be..4226df5  main -> main\n",
      "\n",
      "To https://huggingface.co/irodrigues/my_awesome_model\n",
      "   4226df5..1d51f71  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/irodrigues/my_awesome_model/commit/4226df5cdba03134a1f8069debe7470b701e2486'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf844216",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) applies dynamic padding by default when you pass `tokenizer` to it. In this case, you don't need to specify a data collator explicitly.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2dafd9",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "For a more in-depth example of how to finetune a model for text classification, take a look at the corresponding\n",
    "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)\n",
    "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1fd0f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49368a1b",
   "metadata": {},
   "source": [
    "Great, now that you've finetuned a model, you can use it for inference!\n",
    "\n",
    "Grab some text you'd like to run inference on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39802e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:03:27.721021Z",
     "iopub.status.busy": "2023-05-21T14:03:27.720736Z",
     "iopub.status.idle": "2023-05-21T14:03:27.730629Z",
     "shell.execute_reply": "2023-05-21T14:03:27.729583Z",
     "shell.execute_reply.started": "2023-05-21T14:03:27.720994Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8ba49",
   "metadata": {},
   "source": [
    "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for sentiment analysis with your model, and pass your text to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1906dd51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:03:43.278102Z",
     "iopub.status.busy": "2023-05-21T14:03:43.277082Z",
     "iopub.status.idle": "2023-05-21T14:03:56.693604Z",
     "shell.execute_reply": "2023-05-21T14:03:56.692588Z",
     "shell.execute_reply.started": "2023-05-21T14:03:43.278064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41dbbac650264c018684501bd6973fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61f4a79a8cf4935aa77a392655f70c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0417cd44974f39826da5d6d32fe975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6140aca013d7480d997b77d614bbd13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0af0db9580e4846906a6f1aa844a31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9510816c86164fffa0559f2681cd74d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.991104781627655}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"irodrigues/my_awesome_model\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d8ec6",
   "metadata": {},
   "source": [
    "You can also manually replicate the results of the `pipeline` if you'd like:\n",
    "\n",
    "Tokenize the text and return PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05417668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:03:57.180656Z",
     "iopub.status.busy": "2023-05-21T14:03:57.179926Z",
     "iopub.status.idle": "2023-05-21T14:03:57.347277Z",
     "shell.execute_reply": "2023-05-21T14:03:57.346128Z",
     "shell.execute_reply.started": "2023-05-21T14:03:57.180616Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"irodrigues/my_awesome_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72425105",
   "metadata": {},
   "source": [
    "Pass your inputs to the model and return the `logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "244cb0e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:04:19.069035Z",
     "iopub.status.busy": "2023-05-21T14:04:19.068478Z",
     "iopub.status.idle": "2023-05-21T14:04:19.076799Z",
     "shell.execute_reply": "2023-05-21T14:04:19.075892Z",
     "shell.execute_reply.started": "2023-05-21T14:04:19.069001Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a090ead6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:04:20.475935Z",
     "iopub.status.busy": "2023-05-21T14:04:20.475510Z",
     "iopub.status.idle": "2023-05-21T14:04:21.769325Z",
     "shell.execute_reply": "2023-05-21T14:04:21.766288Z",
     "shell.execute_reply.started": "2023-05-21T14:04:20.475895Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"irodrigues/my_awesome_model\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0aeb04",
   "metadata": {},
   "source": [
    "Get the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6838fd13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-21T14:04:24.832260Z",
     "iopub.status.busy": "2023-05-21T14:04:24.831863Z",
     "iopub.status.idle": "2023-05-21T14:04:24.846079Z",
     "shell.execute_reply": "2023-05-21T14:04:24.844977Z",
     "shell.execute_reply.started": "2023-05-21T14:04:24.832221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".tm_env",
   "language": "python",
   "name": ".tm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
